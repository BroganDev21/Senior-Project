{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129aad43-b046-4773-a03b-3f68e68c4199",
   "metadata": {},
   "source": [
    "# Senior Project: AI vs Human Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340ba3f-5e60-4e9e-9467-27b22d1c4420",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8c6deb-d346-48da-a7dc-1b5eec03aef9",
   "metadata": {},
   "source": [
    "### Currently using my old writing assignments and some random essays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496b737-08d5-41cf-bcad-1dd876745af4",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2602c43b-fae1-4337-a7ab-689eea77c3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-5.1.0-cp38-cp38-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\colin\\anaconda3\\envs\\senior_project\\lib\\site-packages (from python-docx) (4.9.0)\n",
      "Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.6 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/239.6 kB 640.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 239.6/239.6 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading lxml-5.1.0-cp38-cp38-win_amd64.whl (3.9 MB)\n",
      "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.1/3.9 MB 44.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.9/3.9 MB 49.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml, python-docx\n",
      "Successfully installed lxml-5.1.0 python-docx-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dd740-b17b-4a9a-b300-52b7d89836a5",
   "metadata": {},
   "source": [
    "## Setup For Metadata Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110fbb41-d183-43f3-824b-27519a03361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def get_sentence_count(text):\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    return len([s for s in sentences if s.strip()])\n",
    "\n",
    "def get_avg_word_length(text):\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_paragraph_count(text):\n",
    "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by double newlines\n",
    "    return len([p for p in paragraphs if p.strip()])\n",
    "\n",
    "def get_lexical_diversity(text):\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    if words:\n",
    "        return len(unique_words) / len(words)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6163a139-fd9c-4ba2-b077-d4ddbb231249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document data exported to 'combined_docs_with_metadata.csv'.\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def docx_to_text_and_metadata(path, label):\n",
    "    doc = Document(path)\n",
    "    full_text = [paragraph.text for paragraph in doc.paragraphs]\n",
    "    text = '\\n'.join(full_text)\n",
    "    \n",
    "    # Accessing document properties\n",
    "    title = doc.core_properties.title\n",
    "    subject = doc.core_properties.subject\n",
    "    \n",
    "    # Calculate additional textual metadata\n",
    "    word_count = get_word_count(text)\n",
    "    sentence_count = get_sentence_count(text)\n",
    "    avg_word_length = get_avg_word_length(text)\n",
    "    paragraph_count = get_paragraph_count(text)\n",
    "    lexical_diversity = get_lexical_diversity(text)\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'title': title,\n",
    "        'subject': subject,\n",
    "        'word_count': word_count,\n",
    "        'sentence_count': sentence_count,\n",
    "        'avg_word_length': avg_word_length,\n",
    "        'paragraph_count': paragraph_count,\n",
    "        'lexical_diversity': lexical_diversity,\n",
    "        'file_name': os.path.basename(path),  # Get the file name directly from the path\n",
    "        'label': label  # Include the label in the returned data\n",
    "    }\n",
    "\n",
    "def process_essays_directory(docs_dir, label):\n",
    "    doc_files = [f for f in os.listdir(docs_dir) if f.endswith('.docx')]\n",
    "    docs_data = []\n",
    "\n",
    "    for file_name in doc_files:\n",
    "        file_path = os.path.join(docs_dir, file_name)\n",
    "        doc_data = docx_to_text_and_metadata(file_path, label)  # Pass the label\n",
    "        docs_data.append(doc_data)\n",
    "\n",
    "    return docs_data\n",
    "\n",
    "# Directories for human-written and AI-generated essays\n",
    "human_docs_dir = r'\\Users\\Colin\\OneDrive\\Desktop\\Human Essays SP'\n",
    "ai_docs_dir = r'\\Users\\Colin\\OneDrive\\Desktop\\AI Essays SP'\n",
    "\n",
    "# Process each directory with the appropriate label\n",
    "human_docs_data = process_essays_directory(human_docs_dir, 'Human')\n",
    "ai_docs_data = process_essays_directory(ai_docs_dir, 'AI')\n",
    "\n",
    "# Combine the data from both sources\n",
    "all_docs_data = human_docs_data + ai_docs_data\n",
    "\n",
    "# Convert to DataFrame and export to CSV\n",
    "df_all_docs = pd.DataFrame(all_docs_data)\n",
    "df_all_docs.to_csv('combined_docs_with_metadata.csv', index=False)\n",
    "print(\"Document data exported to 'combined_docs_with_metadata.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae5b0f-46e0-4e8c-978c-ab6221e411fb",
   "metadata": {},
   "source": [
    "## Find Directory where .csv File is Stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5840c050-25e0-410f-852a-c15b68a7cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Colin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1404d-3437-4add-ac7b-e743300d5999",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
